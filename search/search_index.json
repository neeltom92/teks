{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"tEKS","text":"<ul> <li>Terraform/Terragrunt</li> <li>Contributing</li> <li>Requirements</li> <li>Terragrunt</li> <li>Quickstart</li> <li>Main purposes</li> <li>What you get</li> <li>Curated Features</li> <li>Bottlerocket support</li> <li>AWS Session Manager by default</li> <li>From and to Zero scaling with EKS Managed Node Groups</li> <li>Automatic dependencies upgrade</li> <li>Enforced security</li> <li>Out of the box logging</li> <li>Out of the box monitoring</li> <li>Long term storage with Thanos</li> <li>Support for ARM instances</li> <li>Helm v3 provider</li> <li>Other and not limited to</li> <li>Always up to date</li> <li>Requirements</li> <li>Pre-commit</li> <li>ASDF<ul> <li>Enabling plugins</li> <li>Installing tools</li> </ul> </li> <li>Examples</li> <li>Additional infrastructure blocks</li> <li>Branches</li> <li>License</li> </ul> <p>tEKS is a set of Terraform / Terragrunt modules designed to get you everything you need to run a production EKS cluster on AWS. It ships with sensible defaults, and add a lot of common addons with their configurations that work out of the box.</p> <p>This is our opinionated view of what a well structred infrastructure as code repository should look like.</p> <p> the v5 and further version of this project have been completely revamp and now offer a skeleton to use as a base for your infrastructure projects around EKS. All the modules have been moved outside this repository and get their own versioning. The old README is accessible here</p> <p> Terraform implementation will not be maintained anymore because of time, and mostly because it has become quite difficult to get feature parity with Terragrunt. Archive branch is available here</p>"},{"location":"#terraformterragrunt","title":"Terraform/Terragrunt","text":"<ul> <li>Terragrunt implementation is available in the <code>terragrunt</code> folder.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contribution are welcome, as well as issues, we are usually quite reactive. If you need more support for your project, do not hesitate to reach us directly.</p>"},{"location":"#requirements","title":"Requirements","text":""},{"location":"#terragrunt","title":"Terragrunt","text":"<ul> <li>Terraform</li> <li>Terragrunt</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Quickstart guide is available here or on the official documentation website</p>"},{"location":"#main-purposes","title":"Main purposes","text":"<p>The main goal of this project is to glue together commonly used tooling with Kubernetes/EKS and to get from an AWS Account to a production cluster with everything you need without any manual configuration.</p>"},{"location":"#what-you-get","title":"What you get","text":"<p>A production cluster all defined in IaaC with Terraform/Terragrunt:</p> <ul> <li>AWS VPC if needed based on <code>terraform-aws-vpc</code></li> <li>EKS cluster base on <code>terraform-aws-eks</code></li> <li>Kubernetes addons based on <code>terraform-kubernetes-addons</code>: provides various addons that are often used on Kubernetes and specifically on EKS. This module is currated by Particule and well maintained.</li> </ul> <p>Everything is tied together with Terragrunt and allows you to deploy a multi cluster architecture in a matter of minutes.</p>"},{"location":"#curated-features","title":"Curated Features","text":"<p>The additional features are provided by tEKS here as well as our curated addons module which support a bunch of various configuration.</p>"},{"location":"#bottlerocket-support","title":"Bottlerocket support","text":"<p>Bottlerocket OS is available for node groups (see example here). Bottle rocket is a container centric OS with less attack surface and no default shell.</p>"},{"location":"#aws-session-manager-by-default","title":"AWS Session Manager by default","text":"<p>All the instances (Bottlerocket or Amazon Linux) are registered with AWS Session Manager. No SSH keys or SSH access is open on instances. Shell access on every instance can be given with SSM for added security.</p> <pre><code>aws ssm start-session --target INSTANCE_ID\n</code></pre>"},{"location":"#from-and-to-zero-scaling-with-eks-managed-node-groups","title":"From and to Zero scaling with EKS Managed Node Groups","text":"<p>tEKS support scaling to and from 0, even with using well know Kubernetes labels, there are a number of ongoing issues for support of EKS Managed node groups with Cluster Autoscaler. Thanks to automatic ASG tagging, tEKS adds the necessary tags on autoscaling group to balance similar node groups and allow you to scale to and from 0 and even to use well know labels such as <code>node.kubernetes.io/instance-type</code> or <code>topology.kubernetes.io/zone</code>. The logic can be extended to support other well known labels.</p>"},{"location":"#automatic-dependencies-upgrade","title":"Automatic dependencies upgrade","text":"<p>We are using renovate to automatically open PR with the latest dependencies update (Terraform modules upgrade) so you never miss an upgrade and are alwasy up to date with the latest features.</p>"},{"location":"#enforced-security","title":"Enforced security","text":"<ul> <li>Encryption by default for root volume on instances with Custom KMS Key</li> <li>AWS EBS CSI volumes encrypted by default with Custom KMS Key</li> <li>No IAM credentials on instances, everything is enforced with IRSA.</li> <li>Each addons is deployed in it's own namespace with sensible default network policies.</li> <li>Calico Tigera Operator for network policy.</li> <li>PSP are enabled but not enforced because of depreciation.</li> </ul>"},{"location":"#out-of-the-box-logging","title":"Out of the box logging","text":"<p>Three stacks are supported: * AWS for Fluent Bit: Forward containers logs to Cloudwatch Logs * Grafana Loki: Uses Promtail to forward logs     to Loki. Grafana or a tEKS supported     monitoring stack (see below) is necessary to display logs.</p>"},{"location":"#out-of-the-box-monitoring","title":"Out of the box monitoring","text":"<ul> <li>Prometheus Operator with defaults dashboards</li> <li>Addons that support metrics are enable along with their <code>serviceMonitor</code></li> <li>Custom grafana dashboard are available by default</li> </ul> <p>Two stacks are supported: * Victoria Metrics Stack: Victoria Metrics is a Prometheus alertnative, compatible with prometheus CRDs * Kube Prometheus Stack: Classic Prometheus Monitoring</p>"},{"location":"#long-term-storage-with-thanos","title":"Long term storage with Thanos","text":"<p>With Prometheus, tEKS includes Thanos by default. Thanos uses S3 to store and query metrics, offering long term storage without the costs. For more information check out our article on the CNCF Blog</p>"},{"location":"#support-for-arm-instances","title":"Support for ARM instances","text":"<p>With either Amazon Linux or BottleRocket, you can use a mix of ARM and AMD64 instances. Check out our example</p>"},{"location":"#helm-v3-provider","title":"Helm v3 provider","text":"<ul> <li>All addons support Helm v3 configuration</li> <li>All charts are easily customizable</li> </ul>"},{"location":"#other-and-not-limited-to","title":"Other and not limited to","text":"<ul> <li>priorityClasses for addons and critical addons</li> <li>lot of manual stuff have been automated under the hood</li> </ul>"},{"location":"#always-up-to-date","title":"Always up to date","text":"<p>We always support the latest modules and features for our addons module.</p> <p>Our cutting edges addons include (not limited to):   * AWS EBS CSI Drivers: Support for Volume encryption by default, snapshot, etc   * AWS EFS CSI Drivers: Use AWS NFS shares.   * Secret Store CSI Driver: load       secret from Secret Managers with       <code>aws-secret-store-csi-driver</code> driver   * Linkerd2 or Certificate Manager CSI for mTLS</p>"},{"location":"#requirements_1","title":"Requirements","text":"<p>Terragrunt is not a hard requirement but all the modules are tested with Terragrunt.</p> <ul> <li>Terraform</li> <li>Terragrunt</li> <li>kubectl</li> <li>helm</li> </ul>"},{"location":"#pre-commit","title":"Pre-commit","text":"<p>This repository use pre-commit hooks, please see this on how to setup tooling</p>"},{"location":"#asdf","title":"ASDF","text":"<p>ASDF is a package manager which is great for managing cloud native tooling. More info here (eg. French).</p>"},{"location":"#enabling-plugins","title":"Enabling plugins","text":"<pre><code>for p in $(cut -d \" \" .tool-versions -f1); do asdf plugin add $p; done\n</code></pre>"},{"location":"#installing-tools","title":"Installing tools","text":"<pre><code>asdf install\n</code></pre>"},{"location":"#examples","title":"Examples","text":"<p><code>terragrunt/live</code> folder provides an opinionated directory structure for a production environment.</p>"},{"location":"#additional-infrastructure-blocks","title":"Additional infrastructure blocks","text":"<p>If you wish to extend your infrastructure you can pick up additional modules on the particuleio github page. Some modules can also be found on the clusterfrak-dynamics github page.</p>"},{"location":"#branches","title":"Branches","text":"<ul> <li><code>main</code>: Backward incompatible with v1.X but compatible with v2.X, releases bumped to v3.X because a lot has changed.</li> <li><code>release-1.X</code>: Compatible with Terraform &lt; 0.12 and Terragrunt &lt; 0.19. Be sure to target the same modules version.</li> <li><code>release-2.X</code>: Compatible with Terraform &gt;= 0.12 and Terragrunt &gt;= 0.19. Be sure to target the same modules version.</li> </ul>"},{"location":"#license","title":"License","text":""},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#cloud-requirements","title":"Cloud Requirements","text":"<ul> <li>At least one AWS Account with <code>AdministratorAccess</code></li> <li>AWS CLI configured with the account you want to deploy into</li> <li>An AWS route53 domain name if you want default Ingresses to just work.     box. It is fine without it but External DNS and Cert Manager won't work out     of the box</li> </ul>"},{"location":"quickstart/#dependencies","title":"Dependencies","text":"<p>Dependencies can be found in <code>.tools-version</code> this file is compatible with asdf which is not a hard requirement but our way of managing required tooling.</p>"},{"location":"quickstart/#enabling-plugins","title":"Enabling plugins","text":"<pre><code>for p in $(cut -d \" \" .tool-versions -f1); do asdf plugin add $p; done\n</code></pre>"},{"location":"quickstart/#installing-tools","title":"Installing tools","text":"<pre><code>asdf install\n</code></pre>"},{"location":"quickstart/#create-repository-structure","title":"Create repository structure","text":"<ol> <li> <p>Clone the template on Github</p> </li> <li> <p>In <code>./terragrunt/live/global_values</code> adapt to your requirements</p> </li> </ol> <pre><code>---\n# Your AWS Account ID\naws_account_id: 161285725140\n\n# Prefix to be added to created resources\nprefix: pio-teks\n\n# AWS S3 bucket region where Terraform will store state\ntf_state_bucket_region: eu-west-1\n\n# Github username or organization, this can be used by Flux2 to auto configure\n# Github bootstrap\ngithub_owner: particuleio\n</code></pre> <ol> <li>In <code>./terragrunt/live/production/env_values.yaml</code> adapt to your requirements,    it is also possible to override variables defined in <code>global_values.yaml</code>    here, for example when using different AWS account per environment. Here we    will use only one AWS account and deploy the production environment.</li> </ol> <pre><code>---\n# Environment name, normally equal to folder name, here it is production by default\nenv: production\n\n# Default domain name that will be used by default ingress resources, use a registered Route53 domain in the AWS Account\ndefault_domain_name: clusterfrak-dynamics.io\n</code></pre> <ol> <li>In <code>terragrunt/live/production/eu-west-1/region_values.yaml</code> there is nothing    to change if you want to use the example region (<code>eu-west-1</code>), if you want to    use another region, just rename the folder, for example <code>us-east-1</code> and then    edit <code>region_values.yaml</code> to suit your need.</li> </ol> <pre><code>---\naws_region: eu-west-1\n</code></pre> <ol> <li> <p>In    <code>terragrunt/live/production/eu-west-1/clusters/demo/component_values.yaml</code>,    <code>name</code> will be used to compute full cluster name, the default is    <code>$PREFIX-$ENV_$NAME</code> which is defined    here.    It is of course possible to override default variable inside the respective    <code>terragrunt.hcl</code> files</p> </li> <li> <p>You can edit each modules individually inside    <code>terragrunt/live/production/eu-west-1/clusters/demo</code>.    For official modules, please refer to their respective documentations. For    <code>eks-addons</code> you can check the module here.</p> </li> <li> <p>Configure Flux2 Gitops in    <code>terragrunt/live/production/eu-west-1/clusters/demo/eks-addons/terragrunt.hcl</code> or disable it if needed, you will need a    GITHUB_TOKEN available from you terminal. Also to configure it according the    your repository name.</p> </li> </ol> <pre><code>  # For this to work:\n  # * GITHUB_TOKEN should be set\n  flux2 = {\n    enabled               = true\n    target_path           = \"gitops/clusters/${include.root.locals.merged.env}/${include.root.locals.merged.name}\"\n    github_url            = \"ssh://git@github.com/${include.root.locals.merged.github_owner}/teks\"\n    repository            = \"teks\"\n    branch                = \"main\"\n    repository_visibility = \"public\"\n    version               = \"v0.25.3\"\n    auto_image_update     = true\n  }\n</code></pre> <ol> <li>Make sure you AWS credential are correctly loaded inside your terminal, then    from the <code>terragrunt/live/production/eu-west-1/clusters/demo</code>.</li> </ol> <pre><code>terragrunt run-all apply\n\n\nINFO[0000] The stack at /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo will be processed in the following order for command apply:\nGroup 1\n- Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/encryption-config\n- Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/vpc\n\nGroup 2\n- Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/eks\n- Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/vpc-endpoints\n\nGroup 3\n- Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/aws-auth\n- Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/eks-addons-critical\n\nGroup 4\n- Module /home/klefevre/git/archifleks/teks-quickstart/terragrunt/live/production/eu-west-1/clusters/demo/eks-addons\n</code></pre> <ol> <li>Load Kubeconfig, you still need to have the AWS CLI loaded and configure with    the right account</li> </ol> <pre><code>export KUBECONFIG=$PWD/eks/kubeconfig\n</code></pre> <ol> <li>Check out ingress objects</li> </ol> <pre><code>k get ingress --all-namespaces\nNAMESPACE    NAME                            CLASS   HOSTS                               ADDRESS                                                                         PORTS     AGE\nmonitoring   kube-prometheus-stack-grafana   nginx   telemetry.clusterfrak-dynamics.io   k8s-ingressn-ingressn-d192ac60af-c080dd921f212013.elb.eu-west-1.amazonaws.com   80, 443   12m\n</code></pre> <ol> <li>Log into Grafana. From the <code>eks-addons</code> folder</li> </ol> <pre><code>terragrunt output grafana_password\n\"PASSWORD\"\n</code></pre> <ol> <li> <p>Use the cluster to do stuff you normally do on a Kubernetes Cluster</p> </li> <li> <p>To destroy everything simply run <code>terragrunt run-all destroy --terragrunt-exclude-dir=aws-auth</code> from the     <code>eu-west-1/clusters/demo</code> folder.</p> </li> </ol> <p> there is an issue with flux 2 namespace not terminating correctly because CRDs are deleted before namespace is terminated. To unstuck <code>flux-system</code> namespace deletion, you can run the following command:</p> <pre><code>kubectl get namespace \"flux-system\" -o json | tr -d \"\\n\" | sed \"s/\\\"finalizers\\\": \\[[^]]\\+\\]/\\\"finalizers\\\": []/\" | kubectl replace --raw /api/v1/namespaces/flux-system/finalize -f -\n</code></pre> <ol> <li>Verify everything is deleted on AWS console (I just did not want the     quickstart to end on an odd number)</li> </ol>"},{"location":"user-guides/eks-addons/","title":"EKS addons module","text":"<p><code>terraform-kubernetes-addons:aws</code> is a custom module maintained here and provides:</p> <ul> <li>helm v3 charts</li> <li>manifests</li> <li>operators</li> </ul> <p>For commonly used addons one Kubernetes and most specifically with EKS.</p> <p>The configuration is curated to be tightly integrated with AWS and EKS.</p>"},{"location":"user-guides/eks-addons/#customization","title":"Customization","text":"<p>All the configuration is done in <code>eks-addons/terragrunt.hcl</code>.</p> <pre><code>dependencies {\npaths = [\"../eks-addons-critical\"]\n}\n\ninclude \"root\" {\npath           = find_in_parent_folders()\nexpose         = true\nmerge_strategy = \"deep\"\n}\n\ninclude \"vpc\" {\npath           = \"../../../../../../dependency-blocks/vpc.hcl\"\nexpose         = true\nmerge_strategy = \"deep\"\n}\n\ninclude \"eks\" {\npath           = \"../../../../../../dependency-blocks/eks.hcl\"\nexpose         = true\nmerge_strategy = \"deep\"\n}\n\nterraform {\nsource = \"github.com/particuleio/terraform-kubernetes-addons.git//modules/aws?ref=v12.8.0\"\n}\n\ngenerate \"provider-local\" {\npath      = \"provider-local.tf\"\nif_exists = \"overwrite\"\ncontents  = file(\"../../../../../../provider-config/eks-addons/eks-addons.tf\")\n}\n\ngenerate \"provider-github\" {\npath      = \"provider-github.tf\"\nif_exists = \"overwrite_terragrunt\"\ncontents  = &lt;&lt;-EOF\nprovider \"github\" {\nowner = \"${include.root.locals.merged.github_owner}\"\n}\nEOF\n}\n\ninputs = {\n\npriority-class = {\nname = basename(get_terragrunt_dir())\n}\n\npriority-class-ds = {\nname = \"${basename(get_terragrunt_dir())}-ds\"\n}\n\ncluster-name = dependency.eks.outputs.cluster_name\n\ntags = merge(\ninclude.root.locals.custom_tags\n)\n\neks = {\n\"cluster_oidc_issuer_url\" = dependency.eks.outputs.cluster_oidc_issuer_url\n\"oidc_provider_arn\"       = dependency.eks.outputs.oidc_provider_arn\n\"cluster_endpoint\"        = dependency.eks.outputs.cluster_endpoint\n}\n\ncert-manager = {\nenabled                   = true\nacme_http01_enabled       = true\nacme_dns01_enabled        = true\nacme_http01_ingress_class = \"nginx\"\nextra_values              = &lt;&lt;-EXTRA_VALUES\ningressShim:\ndefaultIssuerName: letsencrypt\ndefaultIssuerKind: ClusterIssuer\ndefaultIssuerGroup: cert-manager.io\nEXTRA_VALUES\n}\n\ncluster-autoscaler = {\nenabled      = false\nversion      = \"v1.24.0\"\nextra_values = &lt;&lt;-EXTRA_VALUES\nextraArgs:\nscale-down-utilization-threshold: 0.7\nEXTRA_VALUES\n}\n\n# For this to work:\n# * GITHUB_TOKEN should be set\nflux2 = {\nenabled               = false\ntarget_path           = \"gitops/clusters/${include.root.locals.merged.env}/${include.root.locals.merged.name}\"\ngithub_url            = \"ssh://git@github.com/particuleio/teks\"\nrepository            = \"teks\"\nbranch                = \"flux\"\nrepository_visibility = \"public\"\nversion               = \"v0.37.0\"\nauto_image_update     = true\n}\n\nkube-prometheus-stack = {\nenabled                           = true\nallowed_cidrs                     = dependency.vpc.outputs.intra_subnets_cidr_blocks\nthanos_sidecar_enabled            = true\nthanos_bucket_force_destroy       = true\ngrafana_create_iam_resources_irsa = true\nextra_values                      = &lt;&lt;-EXTRA_VALUES\ngrafana:\nimage:\ntag: 9.3.1\ndeploymentStrategy:\ntype: Recreate\ningress:\nannotations:\nkubernetes.io/tls-acme: \"true\"\ningressClassName: nginx\nenabled: true\nhosts:\n- telemetry.${include.root.locals.merged.default_domain_name}\ntls:\n- secretName: ${include.root.locals.merged.default_domain_name}\nhosts:\n- telemetry.${include.root.locals.merged.default_domain_name}\npersistence:\nenabled: true\naccessModes:\n- ReadWriteOnce\nsize: 1Gi\nprometheus:\nprometheusSpec:\nnodeSelector:\nkubernetes.io/arch: amd64\nscrapeInterval: 60s\nretention: 2d\nretentionSize: \"10GB\"\nruleSelectorNilUsesHelmValues: false\nserviceMonitorSelectorNilUsesHelmValues: false\npodMonitorSelectorNilUsesHelmValues: false\nprobeSelectorNilUsesHelmValues: false\nstorageSpec:\nvolumeClaimTemplate:\nspec:\naccessModes: [\"ReadWriteOnce\"]\nresources:\nrequests:\nstorage: 10Gi\nresources:\nrequests:\ncpu: 1\nmemory: 2Gi\nlimits:\ncpu: 2\nmemory: 2Gi\nEXTRA_VALUES\n}\n\nloki-stack = {\nenabled              = true\nbucket_force_destroy = true\nextra_values         = &lt;&lt;-VALUES\nresources:\nrequests:\ncpu: 1\nmemory: 2Gi\nlimits:\ncpu: 2\nmemory: 4Gi\nloki:\nlimits_config:\ningestion_rate_mb: 320\ningestion_burst_size_mb: 512\nmax_streams_per_user: 100000\nchunk_store_config:\nmax_look_back_period: 2160h\ntable_manager:\nretention_deletes_enabled: true\nretention_period: 2160h\ningress:\nenabled: true\nannotations:\nkubernetes.io/tls-acme: \"true\"\nnginx.ingress.kubernetes.io/auth-tls-verify-client: \"on\"\nnginx.ingress.kubernetes.io/auth-tls-secret: \"telemetry/loki-ca\"\nhosts:\n- logz.${include.root.locals.merged.default_domain_name}\ntls:\n- secretName: logz.${include.root.locals.merged.default_domain_name}\nhosts:\n- logz.${include.root.locals.merged.default_domain_name}\nVALUES\nbucket_lifecycle_rule = [\n{\nid      = \"log\"\nenabled = true\ntransition = [\n{\ndays          = 14\nstorage_class = \"INTELLIGENT_TIERING\"\n},\n]\nexpiration = {\ndays = 365\n}\n},\n]\n}\n\ningress-nginx = {\nenabled       = true\nuse_nlb_ip    = true\nallowed_cidrs = dependency.vpc.outputs.intra_subnets_cidr_blocks\nextra_values  = &lt;&lt;-EXTRA_VALUES\ncontroller:\ningressClassResource:\nenabled: true\ndefault: true\nreplicaCount: 2\nminAvailable: 1\nkind: \"Deployment\"\nresources:\nrequests:\ncpu: 300m\nmemory: 128Mi\nservice:\nannotations:\nservice.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true\ndefaultBackend:\nenabled: true\nreplicaCount: 2\nminAvailable: 0\nnodeSelector:\nkubernetes.io/arch: amd64\nEXTRA_VALUES\n}\n\nkarpenter = {\nenabled      = true\niam_role_arn = dependency.eks.outputs.eks_managed_node_groups[\"initial\"].iam_role_arn\n}\n\npromtail = {\nenabled = true\nwait    = false\n}\n\nthanos = {\nenabled              = true\nbucket_force_destroy = true\n# Waiting for ARM support https://github.com/bitnami/charts/issues/7305\nextra_values = &lt;&lt;-EXTRA_VALUES\nquery:\nnodeSelector:\nkubernetes.io/arch: amd64\nqueryFrontend:\nnodeSelector:\nkubernetes.io/arch: amd64\nbucketweb:\nnodeSelector:\nkubernetes.io/arch: amd64\ncompactor:\nnodeSelector:\nkubernetes.io/arch: amd64\nstoregateway:\nnodeSelector:\nkubernetes.io/arch: amd64\nruler:\nnodeSelector:\nkubernetes.io/arch: amd64\nreceive:\nnodeSelector:\nkubernetes.io/arch: amd64\nreceiveDistributor:\nnodeSelector:\nkubernetes.io/arch: amd64\nEXTRA_VALUES\n}\n}\n</code></pre>"},{"location":"user-guides/eks-addons/#default-charts-values","title":"Default charts values","text":"<p>Some values are defined by default directly into the module. These can off course be overridden and or merged/replaced. You can find the defaults values in the upstream module. Eg. default values for <code>cluster-autoscaler</code> are in <code>cluster-autoscaler.tf</code>.</p>"},{"location":"user-guides/eks-addons/#overriding-helm-provider-values","title":"Overriding Helm provider values","text":"<p>Helm provider have defaults values defined here:</p> <pre><code>  helm_defaults_defaults = {\natomic                = false\ncleanup_on_fail       = false\ndependency_update     = false\ndisable_crd_hooks     = false\ndisable_webhooks      = false\nforce_update          = false\nrecreate_pods         = false\nrender_subchart_notes = true\nreplace               = false\nreset_values          = false\nreuse_values          = false\nskip_crds             = false\ntimeout               = 3600\nverify                = false\nwait                  = true\nextra_values          = \"\"\n}\n</code></pre> <p>These can be overridden globally with the <code>helm_defaults</code> input variable or can be overridden per chart in <code>terragrunt.hcl</code>:</p> <pre><code>  helm_defaults = {\nreplace = true\nverify  = true\ntimeout = 300\n}\n\n\ncluster_autoscaler = {\ncreate_iam_resources_irsa = true\niam_policy_override       = \"\"\nversion                   = \"v1.14.7\"\nchart_version             = \"6.4.0\"\nenabled                   = true\ndefault_network_policy    = true\ncluster_name              = dependency.eks.outputs.cluster_id\ntimeout                   = 3600 &lt;= here you can add any helm provider override\n}\n</code></pre>"},{"location":"user-guides/eks-addons/#overriding-charts-valuesyaml","title":"Overriding charts values.yaml","text":"<p>It is possible to add or override values per charts. Helm provider use the same merge logic as Helm so you can basically rewrite the whole <code>values.yaml</code> if needed.</p> <p>Each chart has a <code>extra_values</code> variable where you can specify custom values.</p> <pre><code>flux = {\ncreate_iam_resources_irsa = true\nversion                   = \"1.18.0\"\nchart_version             = \"1.2.0\"\nenabled                   = false\ndefault_network_policy    = true\n\nextra_values = &lt;&lt;EXTRA_VALUES\ngit:\nurl: \"ssh://git@gitlab.com/myrepo/gitops-${local.env}.git\"\npollInterval: \"2m\"\nrbac:\ncreate: false\nregistry:\nautomationInterval: \"2m\"\nEXTRA_VALUES\n}\n</code></pre> <p>There are some examples in the <code>terragrunt.hcl</code> file. Not all the variables available are present. If you want a full list of variable, you can find them in the upstream module. For example for <code>cluster-autoscaler</code> you can see the default here.</p>"},{"location":"user-guides/eks/","title":"EKS","text":"<p>EKS module</p>"},{"location":"user-guides/eks/#upstream-configuration","title":"Upstream configuration","text":"<p>EKS module is also upstream and allow to deploy an EKS cluster which supports:</p> <ul> <li>managed node pools</li> <li>self managed node groups using launch template</li> </ul> <p>tEKS uses EKS managed node groups by default and use one node pool per availability zone.</p> <p>You can use any inputs from the upstream module to configure the cluster in <code>eks/terragrunt.hcl</code>.</p> <p>See all available feature here</p> <pre><code>include \"root\" {\npath           = find_in_parent_folders()\nexpose         = true\nmerge_strategy = \"deep\"\n}\n\ninclude \"vpc\" {\npath           = \"../../../../../../dependency-blocks/vpc.hcl\"\nexpose         = true\nmerge_strategy = \"deep\"\n}\n\ninclude \"ebs_encryption\" {\npath           = \"../../../../../../dependency-blocks/ebs-encryption.hcl\"\nexpose         = true\nmerge_strategy = \"deep\"\n}\n\nlocals {\naws_vpc_cni_version = \"1.12.1\"\ncluster_name        = include.root.locals.full_name\n\nmng_tags = merge(\ninclude.root.locals.custom_tags,\n)\n}\n\nterraform {\nsource = \"github.com/terraform-aws-modules/terraform-aws-eks?ref=v19.15.2\"\n\nafter_hook \"kubeconfig\" {\ncommands = [\"apply\"]\nexecute  = [\"bash\", \"-c\", \"aws eks update-kubeconfig --name ${include.root.locals.full_name} --kubeconfig ${get_terragrunt_dir()}/kubeconfig 2&gt;/dev/null\"]\n}\n\nafter_hook \"kube-system-label\" {\ncommands = [\"apply\"]\nexecute  = [\"bash\", \"-c\", \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig label ns kube-system name=kube-system --overwrite\"]\n}\n\nafter_hook \"undefault-gp2\" {\ncommands = [\"apply\"]\nexecute  = [\"bash\", \"-c\", \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig patch storageclass gp2 -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"false\\\"}}}'\"]\n}\n\nafter_hook \"vpc-cni-prefix-delegation\" {\ncommands = [\"apply\"]\nexecute  = [\"bash\", \"-c\", \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig set env daemonset aws-node -n kube-system ENABLE_PREFIX_DELEGATION=true\"]\n}\n\nafter_hook \"vpc-cni-prefix-warm-prefix\" {\ncommands = [\"apply\"]\nexecute  = [\"bash\", \"-c\", \"kubectl --kubeconfig ${get_terragrunt_dir()}/kubeconfig set env daemonset aws-node -n kube-system WARM_PREFIX_TARGET=1\"]\n}\n}\n\ngenerate \"provider-local\" {\npath      = \"provider-local.tf\"\nif_exists = \"overwrite\"\ncontents  = file(\"../../../../../../provider-config/eks/eks.tf\")\n}\n\ninputs = {\n\naws = {\n\"region\" = include.root.locals.merged.aws_region\n}\n\ntags = merge(\ninclude.root.locals.custom_tags\n)\n\nmanage_aws_auth_configmap = true\n\ncluster_name                   = local.cluster_name\ncluster_version                = \"1.26\"\ncluster_enabled_log_types      = [\"api\", \"audit\", \"authenticator\", \"controllerManager\", \"scheduler\"]\ncluster_endpoint_public_access = true\n\ncluster_addons = {\ncoredns = {\naddon_version = \"v1.9.3-eksbuild.2\"\n}\nkube-proxy = {\naddon_version = \"v1.26.2-eksbuild.1\"\n}\nvpc-cni = {\naddon_version = \"v1.12.6-eksbuild.1\"\n}\n}\n\nvpc_id                   = dependency.vpc.outputs.vpc_id\ncontrol_plane_subnet_ids = dependency.vpc.outputs.intra_subnets\n\ncloudwatch_log_group_retention_in_days = 365\n\nnode_security_group_enable_recommended_rules = true\n\nnode_security_group_additional_rules = {\ningress_self_all = {\nfrom_port = 0\nto_port   = 0\nprotocol  = \"-1\"\ntype      = \"ingress\"\nself      = true\n}\ningress_cluster_all = {\nfrom_port                     = 0\nto_port                       = 0\nprotocol                      = \"-1\"\ntype                          = \"ingress\"\nsource_cluster_security_group = true\n}\ningress_node_port_tcp_1 = {\nfrom_port        = 1025\nto_port          = 5472 # Exclude calico-typha port 5473\nprotocol         = \"tcp\"\ntype             = \"ingress\"\ncidr_blocks      = [\"0.0.0.0/0\"]\nipv6_cidr_blocks = [\"::/0\"]\n}\ningress_node_port_tcp_2 = {\nfrom_port        = 5474\nto_port          = 10249 # Exclude kubelet port 10250\nprotocol         = \"tcp\"\ntype             = \"ingress\"\ncidr_blocks      = [\"0.0.0.0/0\"]\nipv6_cidr_blocks = [\"::/0\"]\n}\ningress_node_port_tcp_3 = {\nfrom_port        = 10251\nto_port          = 10255 # Exclude kube-proxy HCHK port 10256\nprotocol         = \"tcp\"\ntype             = \"ingress\"\ncidr_blocks      = [\"0.0.0.0/0\"]\nipv6_cidr_blocks = [\"::/0\"]\n}\ningress_node_port_tcp_4 = {\nfrom_port        = 10257\nto_port          = 61677 # Exclude aws-node port 61678\nprotocol         = \"tcp\"\ntype             = \"ingress\"\ncidr_blocks      = [\"0.0.0.0/0\"]\nipv6_cidr_blocks = [\"::/0\"]\n}\ningress_node_port_tcp_5 = {\nfrom_port        = 61679\nto_port          = 65535\nprotocol         = \"tcp\"\ntype             = \"ingress\"\ncidr_blocks      = [\"0.0.0.0/0\"]\nipv6_cidr_blocks = [\"::/0\"]\n}\ningress_node_port_udp = {\nfrom_port        = 1025\nto_port          = 65535\nprotocol         = \"udp\"\ntype             = \"ingress\"\ncidr_blocks      = [\"0.0.0.0/0\"]\nipv6_cidr_blocks = [\"::/0\"]\n}\n}\n\nnode_security_group_tags = {\n\"karpenter.sh/discovery\" = local.cluster_name\n}\n\neks_managed_node_group_defaults = {\ntags                = local.mng_tags\ndesired_size        = 1\nmin_size            = 1\nmax_size            = 100\ncapacity_type       = \"ON_DEMAND\"\nplatform            = \"bottlerocket\"\nami_release_version = \"1.13.4-f549851d\"\niam_role_additional_policies = {\nadditional = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"\n}\nebs_optimized = true\nupdate_config = {\nmax_unavailable_percentage = 33\n}\nresources = {\nephemeral-storage = \"1Gi\"\n}\nblock_device_mappings = {\nroot = {\ndevice_name = \"/dev/xvda\"\nebs = {\nvolume_size           = 2\nvolume_type           = \"gp3\"\ndelete_on_termination = true\nencrypted             = true\nkms_key_id            = dependency.ebs_encryption.outputs.key_arn\n}\n}\ncontainers = {\ndevice_name = \"/dev/xvdb\"\nebs = {\nvolume_size           = 15\nvolume_type           = \"gp3\"\ndelete_on_termination = true\nencrypted             = true\nkms_key_id            = dependency.ebs_encryption.outputs.key_arn\n}\n}\n}\n}\n\neks_managed_node_groups = {\n\n\"initial\" = {\nami_type                   = \"BOTTLEROCKET_x86_64\"\ninstance_types             = [\"t3a.large\"]\nsubnet_ids                 = dependency.vpc.outputs.private_subnets\nenable_bootstrap_user_data = true\nbootstrap_extra_args       = &lt;&lt;-EOT\n\"max-pods\" = ${run_cmd(\"/bin/sh\", \"-c\", \"../../../../../../../tools/max-pods-calculator.sh --instance-type t3a.large --cni-version ${local.aws_vpc_cni_version} --cni-prefix-delegation-enabled\")}\nEOT\nlabels = {\nnetwork   = \"private\"\nkarpenter = \"false\"\n}\n}\n\n\"default-a\" = {\nami_type                   = \"BOTTLEROCKET_x86_64\"\ninstance_types             = [\"t3a.medium\"]\nsubnet_ids                 = [dependency.vpc.outputs.private_subnets[0]]\nenable_bootstrap_user_data = true\nbootstrap_extra_args       = &lt;&lt;-EOT\n\"max-pods\" = ${run_cmd(\"/bin/sh\", \"-c\", \"../../../../../../../tools/max-pods-calculator.sh --instance-type t3a.medium --cni-version ${local.aws_vpc_cni_version} --cni-prefix-delegation-enabled\")}\nEOT\nlabels = {\nnetwork   = \"private\"\nkarpenter = \"false\"\n}\n}\n\n\"default-b\" = {\nami_type                   = \"BOTTLEROCKET_x86_64\"\ninstance_types             = [\"t3a.medium\"]\nsubnet_ids                 = [dependency.vpc.outputs.private_subnets[1]]\nenable_bootstrap_user_data = true\nbootstrap_extra_args       = &lt;&lt;-EOT\n\"max-pods\" = ${run_cmd(\"/bin/sh\", \"-c\", \"../../../../../../../tools/max-pods-calculator.sh --instance-type t3a.medium --cni-version ${local.aws_vpc_cni_version} --cni-prefix-delegation-enabled\")}\nEOT\nlabels = {\nnetwork   = \"private\"\nkarpenter = \"false\"\n}\n}\n\n\"default-c\" = {\nami_type                   = \"BOTTLEROCKET_x86_64\"\nplatform                   = \"bottlerocket\"\ninstance_types             = [\"t3a.medium\"]\nsubnet_ids                 = [dependency.vpc.outputs.private_subnets[2]]\nenable_bootstrap_user_data = true\nbootstrap_extra_args       = &lt;&lt;-EOT\n\"max-pods\" = ${run_cmd(\"/bin/sh\", \"-c\", \"../../../../../../../tools/max-pods-calculator.sh --instance-type t3a.medium --cni-version ${local.aws_vpc_cni_version} --cni-prefix-delegation-enabled\")}\nEOT\nlabels = {\nnetwork   = \"private\"\nkarpenter = \"false\"\n}\n}\n\n\"arm-a\" = {\nami_type                   = \"BOTTLEROCKET_ARM_64\"\ninstance_types             = [\"t4g.medium\"]\nsubnet_ids                 = [dependency.vpc.outputs.private_subnets[0]]\nenable_bootstrap_user_data = true\nbootstrap_extra_args       = &lt;&lt;-EOT\n\"max-pods\" = ${run_cmd(\"/bin/sh\", \"-c\", \"../../../../../../../tools/max-pods-calculator.sh --instance-type t4g.medium --cni-version ${local.aws_vpc_cni_version} --cni-prefix-delegation-enabled\")}\nEOT\nlabels = {\nnetwork   = \"private\"\nkarpenter = \"false\"\n}\n}\n\n\"arm-b\" = {\nami_type                   = \"BOTTLEROCKET_ARM_64\"\ninstance_types             = [\"t4g.medium\"]\nsubnet_ids                 = [dependency.vpc.outputs.private_subnets[1]]\nenable_bootstrap_user_data = true\nbootstrap_extra_args       = &lt;&lt;-EOT\n\"max-pods\" = ${run_cmd(\"/bin/sh\", \"-c\", \"../../../../../../../tools/max-pods-calculator.sh --instance-type t4g.medium --cni-version ${local.aws_vpc_cni_version} --cni-prefix-delegation-enabled\")}\nEOT\nlabels = {\nnetwork   = \"private\"\nkarpenter = \"false\"\n}\n}\n\n\"arm-c\" = {\nami_type                   = \"BOTTLEROCKET_ARM_64\"\ninstance_types             = [\"t4g.medium\"]\nsubnet_ids                 = [dependency.vpc.outputs.private_subnets[2]]\nenable_bootstrap_user_data = true\nbootstrap_extra_args       = &lt;&lt;-EOT\n\"max-pods\" = ${run_cmd(\"/bin/sh\", \"-c\", \"../../../../../../../tools/max-pods-calculator.sh --instance-type t4g.medium --cni-version ${local.aws_vpc_cni_version} --cni-prefix-delegation-enabled\")}\nEOT\nlabels = {\nnetwork   = \"private\"\nkarpenter = \"false\"\n}\n}\n}\n}\n</code></pre>"},{"location":"user-guides/getting-started/","title":"Getting started","text":""},{"location":"user-guides/getting-started/#tooling-requirements","title":"Tooling requirements","text":"<p>The necessary tools are in <code>requirements.yaml</code> you can install them any way you want, make sure they are available in your $PATH.</p> <p>The following dependencies are required on the deployer host:</p> <ul> <li>Terraform</li> <li>Terragrunt</li> <li>kubectl</li> <li>helm</li> <li>aws-iam-authenticator</li> </ul>"},{"location":"user-guides/getting-started/#aws-requirements","title":"AWS requirements","text":"<ul> <li>At least one AWS account</li> <li><code>awscli</code> configured (see installation instructions) to access your AWS account.</li> <li>A route53 hosted zone if you plan to use <code>external-dns</code> or <code>cert-manager</code> but it is not a hard requirement.</li> </ul>"},{"location":"user-guides/getting-started/#getting-the-template-repository","title":"Getting the template repository","text":"<p>You can either clone the repo locally or generate/fork a template from github.</p> <pre><code>git clone https://github.com/particuleio/teks.git\n</code></pre> <p>The terraform directory structure is the following:</p> <pre><code>.\n\u2514\u2500\u2500 live\n    \u251c\u2500\u2500 backend\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 backend.tf\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 providers.tf\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 state.tf\n    \u251c\u2500\u2500 demo\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 env_tags.yaml\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 eu-west-3\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 clusters\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 full\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 eks\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 aws-provider.tf -&gt; ../../../../../shared/aws-provider.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 backend.tf -&gt; ../../../../../backend/backend.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 data.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 locals.tf -&gt; ../../../../../shared/locals.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 eks-addons\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 aws-provider.tf -&gt; ../../../../../shared/aws-provider.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 backend.tf -&gt; ../../../../../backend/backend.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 data.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 locals.tf -&gt; ../../../../../shared/locals.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 versions.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 vpc\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0         \u251c\u2500\u2500 aws-provider.tf -&gt; ../../../../../shared/aws-provider.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0         \u251c\u2500\u2500 backend.tf -&gt; ../../../../../backend/backend.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0         \u251c\u2500\u2500 locals.tf -&gt; ../../../../../shared/locals.tf\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0         \u2514\u2500\u2500 main.tf\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 region_values.yaml\n    \u251c\u2500\u2500 global_tags.yaml\n    \u251c\u2500\u2500 global_values.yaml\n    \u2514\u2500\u2500 shared\n        \u251c\u2500\u2500 aws-provider.tf\n        \u2514\u2500\u2500 locals.tf\n</code></pre> <p>Each cluster in inside the <code>terraform/live</code> folder and then modules are grouped by AWS region.</p>"},{"location":"user-guides/getting-started/#start-a-new-cluster","title":"Start a new cluster","text":"<p>Create a new cluster beside <code>demo</code>:</p> <pre><code>cp -ar demo mycluster\n</code></pre>"},{"location":"user-guides/getting-started/#configuring-the-remote-state","title":"Configuring the remote state","text":"<p>Configuration of the remote state is based on the value of the <code>global_values.yaml</code> file in the <code>terraform</code> and the <code>terragrunt</code> directories based on the installation method you used.</p> <p>Both files are following the same structure.</p> <pre><code>---\nprovider: aws\n# Your AWS Account ID\naws_account_id: 161285725140\n\n# Prefix to be added to created resources\nprefix: pio-teks\n\n# AWS S3 bucket region where Terraform will store state\ntf_state_bucket_region: eu-west-1\n\n# Github username or organization, this can be used by Flux2 to auto configure\n# Github bootstrap\ngithub_owner: particuleio\n</code></pre> <p>Adapt these values to match your configuration (<code>prefix</code>, 'project').</p> <p>Based on the configuration, both methods will create the following resources:</p> <ul> <li>S3 bucket named <code>{prefix}-{project}-{tf|tg}-state</code>: store the state</li> <li>DynamoDB table named <code>{prefix}-{project}-{tf|tg}-state-lock</code>: prevent concurrent use</li> </ul> <p>The resource names will include information based on the configuration method used. Using <code>terraform</code> will create resources with <code>tf</code> in their name, and <code>tg</code> when using <code>terragrunt</code>.</p> <p>Using the current values, the resources created to use <code>terraform</code> will be:</p> <ul> <li>S3: <code>pio-teks-tf-state</code></li> <li>DynamoDB: <code>pio-state-tf-state-lock</code></li> </ul>"},{"location":"user-guides/getting-started/#remote-state-for-terraform","title":"Remote state for Terraform","text":"<p>If you plan on using terraform to setup <code>teks</code>, you need to create your remote backend using cloudposse/terraform-aws-tfstate-backend configured in <code>terraform/live/backend/state.tf</code>.</p> <p>In order to configure the S3 backend for terraform, configure your <code>global_values.yaml</code> then go in the <code>terraform/live/backend</code> directory.</p> <ul> <li><code>terraform init</code> init the terraform module.</li> <li><code>terraform apply -auto-approve</code> to create the S3 backend.</li> <li><code>terraform init -force-copy</code> will copy the local backend to the S3 backend.</li> </ul> <p>The <code>terraform-aws-tfstate-backend</code> module will create or update the <code>terraform/live/backend/backend.tf</code> file, which is symlinked to the child modules (<code>vpc</code>, <code>eks</code>, <code>eks-addons</code>).</p> <pre><code>\n</code></pre> <p>Further documentation regarding the remote backend configuration can be found at terraform-aws-tfstate-backend#create.</p>"},{"location":"user-guides/getting-started/#remote-state-for-terragrunt","title":"Remote state for Terragrunt","text":"<p><code>terragrunt/live/demo/terragrunt.hcl</code> is the parent terragrunt file use to configure remote state.</p> <p>The configuration is done automatically based on the <code>terragrunt/live/global_values.yaml</code> file.</p> <p>The values here will generate automatically the parent terragrunt file.</p> <pre><code>\n</code></pre> <p>You can either customize the values or edit directly the <code>terragrunt.hcl</code> file.</p>"},{"location":"user-guides/getting-started/#running-terragrunt-command","title":"Running Terragrunt command","text":"<p>Terragrunt command are run inside their respective folder, for example, to run the <code>vpc</code> module:</p> <pre><code>cd vpc\nterragrunt apply\n</code></pre>"},{"location":"user-guides/vpc/","title":"VPC module","text":"<p>The vpc module is the one from upstream.</p> <p>To customize it. Modify the <code>vpc/terragrunt.hcl</code> file. You can use any inputs available in the upstream module.</p> <pre><code>include \"root\" {\npath           = find_in_parent_folders()\nexpose         = true\nmerge_strategy = \"deep\"\n}\n\nterraform {\nsource = \"github.com/terraform-aws-modules/terraform-aws-vpc?ref=v4.0.1\"\n}\n\ndependency \"datasources\" {\nconfig_path = \"../../../datasources\"\n}\n\nlocals {\nvpc_cidr     = \"10.42.0.0/16\"\nvpc_name     = include.root.locals.full_name\ncluster_name = include.root.locals.full_name\n}\n\ninputs = {\n\ntags = merge(\ninclude.root.locals.custom_tags,\n{\n\"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\",\n}\n)\n\nname = local.vpc_name\ncidr = local.vpc_cidr\nazs  = dependency.datasources.outputs.aws_availability_zones.names\n\nintra_subnets   = [for k, v in slice(dependency.datasources.outputs.aws_availability_zones.names, 0, 3) : cidrsubnet(local.vpc_cidr, 8, k)]\npublic_subnets  = [for k, v in slice(dependency.datasources.outputs.aws_availability_zones.names, 0, 3) : cidrsubnet(local.vpc_cidr, 3, k + 1)]\nprivate_subnets = [for k, v in slice(dependency.datasources.outputs.aws_availability_zones.names, 0, 3) : cidrsubnet(local.vpc_cidr, 3, k + 4)]\n\nenable_ipv6                                    = true\npublic_subnet_ipv6_prefixes                    = [0, 1, 2]\npublic_subnet_assign_ipv6_address_on_creation  = true\nprivate_subnet_ipv6_prefixes                   = [3, 4, 5]\nprivate_subnet_assign_ipv6_address_on_creation = true\nintra_subnet_ipv6_prefixes                     = [6, 7, 8]\nintra_subnet_assign_ipv6_address_on_creation   = true\n\nenable_nat_gateway = true\nsingle_nat_gateway = true\n\nmanage_default_security_group = true\nmap_public_ip_on_launch       = true\n\ndefault_security_group_egress = [\n{\nfrom_port        = 0\nto_port          = 0\nprotocol         = \"-1\"\ncidr_blocks      = \"0.0.0.0/0\"\nipv6_cidr_blocks = \"::/0\"\n}\n]\ndefault_security_group_ingress = [\n{\nfrom_port        = 0\nto_port          = 0\nprotocol         = \"-1\"\ncidr_blocks      = \"0.0.0.0/0\"\nipv6_cidr_blocks = \"::/0\"\n}\n]\n\npublic_subnet_tags = {\n\"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\"\n\"kubernetes.io/role/elb\"                      = \"1\"\n\"karpenter.sh/discovery\"                      = \"true\"\n}\n\nprivate_subnet_tags = {\n\"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\"\n\"kubernetes.io/role/internal-elb\"             = \"1\"\n\"karpenter.sh/discovery\"                      = \"true\"\n}\n\nenable_flow_log                                 = true\ncreate_flow_log_cloudwatch_log_group            = true\ncreate_flow_log_cloudwatch_iam_role             = true\nflow_log_cloudwatch_log_group_retention_in_days = 365\nflow_log_traffic_type                           = \"REJECT\"\n}\n</code></pre>"}]}